{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_data = pd.DataFrame({\n",
    "  'answer': [  0,   1,   1,   0,   1,   0,    1,   0,   0,   1],  # 正解\n",
    "  'prob':   [0.7, 0.8, 0.3, 0.4, 0.9, 0.6, 0.99, 0.1, 0.2, 0.5]}) # 陽性確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pred = [1 if p >= 0.5 else 0 for p in my_data.prob]\n",
    "my_pred\n",
    "#> [1, 1, 0, 0, 1, 1, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(my_pred, my_data.answer) # Rの仕様に合わせる．\n",
    "#> array([[3, 1],\n",
    "#>        [2, 4]])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(my_data.answer, my_pred))\n",
    "#>               precision    recall  f1-score   support\n",
    "#> \n",
    "#>            0       0.75      0.60      0.67         5\n",
    "#>            1       0.67      0.80      0.73         5\n",
    "#> \n",
    "#>     accuracy                           0.70        10\n",
    "#>    macro avg       0.71      0.70      0.70        10\n",
    "#> weighted avg       0.71      0.70      0.70        10"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}
